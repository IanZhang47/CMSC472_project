\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Automated Novel Object Discovery and Detection in Unlabeled Image Datasets}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
	Yinuo Zhang \AND Vance Degen \AND Junyan Dai \AND Andrew McNamara\\
	Department of Computer Science, University of Maryland\\
	College Park, MD 20742, USA\\
	\texttt{\{yzhan123, jdai1234, amcnama2\}@terpmail.umd.edu}}

\date{\today}


\begin{document}
	
	
	\maketitle
	
	
	\begin{abstract}
		The proposal outlines our approach to tackle the object discovery challenge set by OBJ-DISC. We aim to develop an algorithm capable of identifying and grouping semantically coherent objects from large unlabeled datasets and then train an object detector for each identified cluster.
	\end{abstract}
	
	
	\section{Introduction}
	
	
	Object discovery is vital for understanding and interpreting the vast amount of visual data generated daily, leading to advancements in AI's image recognition capabilities. The OBJ-DISC challenge addresses this by focusing on identifying and categorizing new objects within large, unlabeled image datasets. Our project aligns with these objectives, aiming to significantly advance AI's ability to recognize and understand diverse objects, thereby enhancing machine learning models' generalization and application in real-world scenarios.
	
	
	\section{Objectives}

	
	The objectives are to create a system that can group objects in images based on semantic similarity without needing predefined labels, and to validate these clusters by training and assessing object detectors on them, ensuring they can effectively identify these objects in real-world settings.
	
	 \section{Motivation}
	
    Object recognition is very important in understanding visual data, which there are vast amounts of and which increases daily. Given the vastness of visual data, prelabeling objects to allow for training of algorithms becomes less feasible. Therefore there is significant incentive to automate this and reduce the need for human work in labeling image sets. This is why it is important to increase AI's ability to categorize unlabeled objects in datasets, as this will allow human work to focus on other areas. One area where object recognition is important is self driving cars, where the vehicle needs to know if it's approaching something that requires a stop like a person or animal or a non obstacle like a garbage bag. Some negative consequences from object recognition stem from if we have faulty object recognition, such as misidentifying something and causing harm as a result, which is all the more reason to improve object recognition.
	\section{Methodology}
	\label{methodology}
	
	\answerTODO{This is base on some research may need to change later on}
	
	
	\subsection{Object Discovery in Unlabeled Data}
	
	
	Utilize self-supervised learning techniques to discover object clusters within the unlabeled dataset. Techniques such as MOST (Multiple Object localization with Self-supervised Transformers) could be particularly useful here, as they allow for the discovery of multiple objects within images without prior training. The goal would be to segment these images into meaningful clusters that potentially represent different object classes.
	
	\subsection{Feature Extraction and Clustering} 
	
	
	Extract features from these images using pre-trained models or self-supervised methods and then apply clustering algorithms (like K-means, DBSCAN, or hierarchical clustering) to group similar regions together. Each cluster would ideally represent a different object class. The number of clusters, $M$, would not be known a priori and might need to be determined based on the dataset's inherent structure, which can be evaluated using metrics like silhouette scores or the Daviesâ€“Bouldin index.
	
	\subsection{Labeling Known Objects}
	
	
	Leverage the labeled dataset of $K$ known objects to label clusters that closely match these known categories. This could be done by training a supervised classifier on the labeled data and then applying this classifier to the centroids or representative samples of the clusters derived from the unlabeled data.
	
	\subsection{Training Object Detectors}
	
	
	For each of the $M$ clusters, train an object detector using the images in the cluster as positive examples and images from other clusters as negative examples. This step might involve fine-tuning pre-trained object detection models (such as YOLO, SSD, or Faster R-CNN) on clustered datasets.
	
	\subsection{Evaluation}
	
	
	Evaluate the performance of each object detector on a held-out test set. This set should ideally contain both known and unknown objects to assess the real-world applicability of the discovered object classes and their corresponding detectors. Metrics such as precision, recall, F1 score, and mAP (mean Average Precision) could be used for evaluation.
	
	\subsection{Refinement and Iteration}
	
	
	Based on the evaluation, refine the clustering and object detector training processes. This could involve adjusting the number of clusters, changing the feature extraction process, or tuning the object detection models.
	
	This process combines elements of unsupervised learning (clustering and feature extraction from unlabeled data) with supervised learning (using labeled data for initial classifier training and object detector refinement). The success of such a system depends on the quality of the unsupervised object discovery process and the effectiveness of the subsequent object detectors trained for each discovered class.
	
	
	\section{Datasets}
	\label{datasets}
	
	
	Labeled dataset: The object detection dataset, PASCAL VOC 2007 split is considered as the labeled dataset for this challenge. refer to \url{http://host.robots.ox.ac.uk/pascal/VOC/voc2007/}
	
	Discovery Set: The COCO 2014 train set, without any labels, is used as the discovery dataset. The remaining categories, not common with PASCAL-VOC, are considered the novel categories. refer to: \url{https://github.com/cocodataset/cocoapi}
	
	Pre-training dataset: To train object detection datasets, ImageNet pre-training is a standard practice. refer to: \url{https://www.image-net.org/challenges/LSVRC/2012/2012-downloads.php}
	
	Evaluation set: All systems will be evaluated on the discovery performance and object detection performance. For object discovery, results are reported on the COCO 2014 train set. For object detection on the 20 known classes and the newly discovered objects, results will be reported on the COCO minival set.\\
 
 The license for these datasets states: \\Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
	
	\section{Relevant Papers}
 This paper: \href{https://arxiv.org/pdf/1807.05511.pdf}{Object Detection with Deep Learning: A Review} covers a basic overview of how object detection has progressed over time as deep neural networks have advanced, as well as covering a bit of history of non deep learning object detection.\\
 This paper \href{https://ieeexplore.ieee.org/document/10098596}{Object Detection Using Deep Learning, CNNs and Vision Transformers: A Review} covers various methods for object detection that have been used over the years and the concepts underlying advancements in Object detection. It's useful in deepening our understanding of this subject to better implement our approach.
	\section{Expected Outcomes}
	
	
	The expected outcomes are to surpass the performance metrics set by the baseline code provided in the challenge and to apply theoretical knowledge from the course to maximize the efficiency and effectiveness of the algorithm, striving for optimal results in object discovery and detection.
	
	\section{Moonshot Plan}
    The moonshot plan is to get performance matching or surpassing that of a human (recognizing real objects that humans don't). This would require lots of fine tuning and time taken to see how adjustments affect testing performance. It would also require a very deep understanding of the images being evaluated, as it could allow for tuning the algorithm around those features.  It would also be difficult to avoid overfitting when trying to acheive such precise results.
	\section{Timeline and Milestones}
	Proposals due March 15th\\
	Check-ins week of April 11th\\
	Summary slides due May 2nd\\
	Report and presentation due May 9th\\
	\answerTODO{Add intermediate deadline(progress check)}
	
	\section{Team Structure and Responsibilities}
	\answerTODO{specific job for each member}
	
	\section{Resources and Tools}
	\answerTODO{base on the algorithm}
	
	\section{Evaluation and Testing}
	$$performance = \frac{\sum_{1}^{N}Purity_{i} \cdot C_{i}}{D}$$
	$$N = \textrm{Total number of clusters}$$
	$$C_{i} = \textrm{Number of elements in the cluster } i$$
	$$Purity_{i} = \textrm{Purity of cluster } i$$
	$$D = \textrm{Total number of instances in the dataset}$$
	
	\section{Conclusion}
	In conclusion, our project aims to advance the field of AI in object discovery and detection by developing a system that outperforms existing baseline models and applying our academic knowledge to optimize the algorithm. Our approach is systematic and grounded in robust methodologies, ensuring compliance with challenge standards while striving for innovation and improved performance in object recognition tasks.
	
	
	
	\section*{References}
	
	
	\answerTODO{Need to be fill in}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section*{Checklist}
	
	
	%%% BEGIN INSTRUCTIONS %%%
	The checklist follows the references.  Please
	read the checklist guidelines carefully for information on how to answer these
	questions.  For each question, change the default \answerTODO{} to \answerYes{},
	\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
		justification to your answer}, either by referencing the appropriate section of
	your paper or providing a brief inline description.  For example:
	\begin{itemize}
		\item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
		\item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
		\item Did you include the license to the code and datasets? \answerNA{}
	\end{itemize}
	Please do not modify the questions and only use the provided macros for your
	answers.  Note that the Checklist section does not count towards the page
	limit.  In your paper, please delete this instructions block and only keep the
	Checklist section heading above along with the questions/answers below.
	%%% END INSTRUCTIONS %%%
	
	
	\begin{enumerate}
		
		
		\item For all authors...
		\begin{enumerate}
			\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?\\
			\answerYes{We cover everything in the abstract throughout the rest of the proposal}
			\item Did you describe the limitations of your work?\\
			\answerYes{We describe in section 7 which is expected outcomes what we expect to accomplish and in section 8 which is the moonhshot plan, the possible although unlikely best results we could get.}
			\item Did you discuss any potential negative societal impacts of your work?\\
			\answerYes{We talk a bit about possible negative societal impacts in the motivation section, such as its possible use for increased surveillance}
			\item Have you read the ethics review guidelines and ensured that your paper conforms to them?\\
			\answerYes{We have}
		\end{enumerate}
		
		
		\item If you are including theoretical results...
		\begin{enumerate}
			\item Did you state the full set of assumptions of all theoretical results?
			\answerNA{}{}
			\item Did you include complete proofs of all theoretical results?
			\answerNA{}
		\end{enumerate}
		
		
		\item If you ran experiments...
		\begin{enumerate}
			\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
			\answerNA{}
			\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
			\answerNA{}
			\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
			\answerNA{}
			\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
			\answerNA{}
		\end{enumerate}
		
		
		\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
		\begin{enumerate}
			\item If your work uses existing assets, did you cite the creators?\\
			\answerYes{We cite the creators in section 5 datasets and in our references}
			\item Did you mention the license of the assets?\\
			\answerYes{We put a copy of the license information in section 5 datasets}
			\item Did you include any new assets either in the supplemental material or as a URL?
			\answerNo{}
			\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?\\
			\answerYes{We cover how the license states we can use the datasets as long as credit is provided, which it will be.}
			\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?\\
			\answerNo{}
		\end{enumerate}
		
		
		\item If you used crowdsourcing or conducted research with human subjects...
		\begin{enumerate}
			\item Did you include the full text of instructions given to participants and screenshots, if applicable?
			\answerNA{}
			\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
			\answerNA{}
			\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
			\answerNA{}
		\end{enumerate}
		
		
	\end{enumerate}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	\appendix
	
	
	\section{Appendix}
	
	
	Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
	This section will often be part of the supplemental material.
	
	
\end{document}
