\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Automated Novel Object Discovery and Detection in Unlabeled Image Datasets}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
	Yinuo Zhang \AND Vance Degen \AND Junyan Dai \AND Andrew McNamara\\
	Department of Computer Science, University of Maryland\\
	College Park, MD 20742, USA\\
	\texttt{\{yzhan123, jdai1234, amcnama2\}@terpmail.umd.edu}}

\date{\today}


\begin{document}
	
	
	\maketitle
	
	
	\begin{abstract}
		The proposal outlines our approach to tackle the object discovery challenge set by OBJ-DISC. We aim to develop an algorithm capable of identifying and grouping semantically coherent objects from large unlabeled datasets and then train an object detector for each identified cluster.
	\end{abstract}
	
	
	\section{Introduction}
	
	
	Object discovery is vital for understanding and interpreting the vast amount of visual data generated daily, leading to advancements in AI's image recognition capabilities. The OBJ-DISC challenge addresses this by focusing on identifying and categorizing new objects within large, unlabeled image datasets. Our project aligns with these objectives, aiming to significantly advance AI's ability to recognize and understand diverse objects, thereby enhancing machine learning models' generalization and application in real-world scenarios.
	
	
	\section{Objectives}

	
	The objectives are to create a system that can group objects in images based on semantic similarity without needing predefined labels, and to validate these clusters by training and assessing object detectors on them, ensuring they can effectively identify these objects in real-world settings.
	
	 \section{Motivation}
	
    Object recognition is very important in understanding visual data, which there are vast amounts of and which increases daily. Given the vastness of visual data, prelabeling objects to allow for training of algorithms becomes less feasible. Therefore there is significant incentive to automate this and reduce the need for human work in labeling image sets. This is why it is important to increase AI's ability to categorize unlabeled objects in datasets, as this will allow human work to focus on other areas.
	\section{Methodology}
	\label{methodology}
	
	\answerTODO{This is base on some research may need to change later on}
	
	
	\subsection{Object Discovery in Unlabeled Data}
	
	
	Utilize self-supervised learning techniques to discover object clusters within the unlabeled dataset. Techniques such as MOST (Multiple Object localization with Self-supervised Transformers) could be particularly useful here, as they allow for the discovery of multiple objects within images without prior training. The goal would be to segment these images into meaningful clusters that potentially represent different object classes.
	
	\subsection{Feature Extraction and Clustering} 
	
	
	Extract features from these images using pre-trained models or self-supervised methods and then apply clustering algorithms (like K-means, DBSCAN, or hierarchical clustering) to group similar regions together. Each cluster would ideally represent a different object class. The number of clusters, $M$, would not be known a priori and might need to be determined based on the dataset's inherent structure, which can be evaluated using metrics like silhouette scores or the Daviesâ€“Bouldin index.
	
	\subsection{Labeling Known Objects}
	
	
	Leverage the labeled dataset of $K$ known objects to label clusters that closely match these known categories. This could be done by training a supervised classifier on the labeled data and then applying this classifier to the centroids or representative samples of the clusters derived from the unlabeled data.
	
	\subsection{Training Object Detectors}
	
	
	For each of the $M$ clusters, train an object detector using the images in the cluster as positive examples and images from other clusters as negative examples. This step might involve fine-tuning pre-trained object detection models (such as YOLO, SSD, or Faster R-CNN) on clustered datasets.
	
	\subsection{Evaluation}
	
	
	Evaluate the performance of each object detector on a held-out test set. This set should ideally contain both known and unknown objects to assess the real-world applicability of the discovered object classes and their corresponding detectors. Metrics such as precision, recall, F1 score, and mAP (mean Average Precision) could be used for evaluation.
	
	\subsection{Refinement and Iteration}
	
	
	Based on the evaluation, refine the clustering and object detector training processes. This could involve adjusting the number of clusters, changing the feature extraction process, or tuning the object detection models.
	
	This process combines elements of unsupervised learning (clustering and feature extraction from unlabeled data) with supervised learning (using labeled data for initial classifier training and object detector refinement). The success of such a system depends on the quality of the unsupervised object discovery process and the effectiveness of the subsequent object detectors trained for each discovered class.
	
	
	\section{Datasets}
	\label{datasets}
	
	
	Labeled dataset: The object detection dataset, PASCAL VOC 2007 split is considered as the labeled dataset for this challenge. refer to \url{http://host.robots.ox.ac.uk/pascal/VOC/voc2007/}
	
	Discovery Set: The COCO 2014 train set, without any labels, is used as the discovery dataset. The remaining categories, not common with PASCAL-VOC, are considered the novel categories. refer to: \url{https://github.com/cocodataset/cocoapi}
	
	Pre-training dataset: To train object detection datasets, ImageNet pre-training is a standard practice. refer to: \url{https://www.image-net.org/challenges/LSVRC/2012/2012-downloads.php}
	
	Evaluation set: All systems will be evaluated on the discovery performance and object detection performance. For object discovery, results are reported on the COCO 2014 train set. For object detection on the 20 known classes and the newly discovered objects, results will be reported on the COCO minival set.
	
	
	\section{Expected Outcomes}
	
	
	The expected outcomes are to surpass the performance metrics set by the baseline code provided in the challenge and to apply theoretical knowledge from the course to maximize the efficiency and effectiveness of the algorithm, striving for optimal results in object discovery and detection.
	
	
	\section{Timeline and Milestones}
	Proposals due March 15th\\
	Check-ins week of April 11th\\
	Summary slides due May 2nd\\
	Report and presentation due May 9th\\
	\answerTODO{Add intermediate deadline(progress check)}
	
	\section{Team Structure and Responsibilities}
	\answerTODO{specific job for each member}
	
	\section{Resources and Tools}
	\answerTODO{base on the algorithm}
	
	\section{Evaluation and Testing}
	$$performance = \frac{\sum_{1}^{N}Purity_{i} \cdot C_{i}}{D}$$
	$$N = \textrm{Total number of clusters}$$
	$$C_{i} = \textrm{Number of elements in the cluster } i$$
	$$Purity_{i} = \textrm{Purity of cluster } i$$
	$$D = \textrm{Total number of instances in the dataset}$$
	
	\section{Conclusion}
	In conclusion, our project aims to advance the field of AI in object discovery and detection by developing a system that outperforms existing baseline models and applying our academic knowledge to optimize the algorithm. Our approach is systematic and grounded in robust methodologies, ensuring compliance with challenge standards while striving for innovation and improved performance in object recognition tasks.
	
	
	
	\section*{References}
	
	
	\answerTODO{Need to be fill in}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section*{Checklist}
	
	
	%%% BEGIN INSTRUCTIONS %%%
	The checklist follows the references.  Please
	read the checklist guidelines carefully for information on how to answer these
	questions.  For each question, change the default \answerTODO{} to \answerYes{},
	\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
		justification to your answer}, either by referencing the appropriate section of
	your paper or providing a brief inline description.  For example:
	\begin{itemize}
		\item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
		\item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
		\item Did you include the license to the code and datasets? \answerNA{}
	\end{itemize}
	Please do not modify the questions and only use the provided macros for your
	answers.  Note that the Checklist section does not count towards the page
	limit.  In your paper, please delete this instructions block and only keep the
	Checklist section heading above along with the questions/answers below.
	%%% END INSTRUCTIONS %%%
	
	
	\begin{enumerate}
		
		
		\item For all authors...
		\begin{enumerate}
			\item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
			\answerTODO{}
			\item Did you describe the limitations of your work?
			\answerTODO{}
			\item Did you discuss any potential negative societal impacts of your work?
			\answerTODO{}
			\item Have you read the ethics review guidelines and ensured that your paper conforms to them?
			\answerTODO{}
		\end{enumerate}
		
		
		\item If you are including theoretical results...
		\begin{enumerate}
			\item Did you state the full set of assumptions of all theoretical results?
			\answerTODO{}
			\item Did you include complete proofs of all theoretical results?
			\answerTODO{}
		\end{enumerate}
		
		
		\item If you ran experiments...
		\begin{enumerate}
			\item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
			\answerTODO{}
			\item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
			\answerTODO{}
			\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
			\answerTODO{}
			\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
			\answerTODO{}
		\end{enumerate}
		
		
		\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
		\begin{enumerate}
			\item If your work uses existing assets, did you cite the creators?
			\answerTODO{}
			\item Did you mention the license of the assets?
			\answerTODO{}
			\item Did you include any new assets either in the supplemental material or as a URL?
			\answerTODO{}
			\item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
			\answerTODO{}
			\item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
			\answerTODO{}
		\end{enumerate}
		
		
		\item If you used crowdsourcing or conducted research with human subjects...
		\begin{enumerate}
			\item Did you include the full text of instructions given to participants and screenshots, if applicable?
			\answerTODO{}
			\item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
			\answerTODO{}
			\item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
			\answerTODO{}
		\end{enumerate}
		
		
	\end{enumerate}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	\appendix
	
	
	\section{Appendix}
	
	
	Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
	This section will often be part of the supplemental material.
	
	
\end{document}
